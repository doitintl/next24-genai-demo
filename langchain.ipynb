{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4adf38a-6179-44e5-84c0-93001d063efd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from opentelemetry import trace\n",
    "from opentelemetry.exporter.cloud_trace import CloudTraceSpanExporter\n",
    "from opentelemetry.sdk.trace import TracerProvider\n",
    "from opentelemetry.sdk.trace.export import BatchSpanProcessor\n",
    "from opentelemetry.trace import Link\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8bd000f-f5ce-4aad-80d8-a9c52d3031a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "052d9b66-573d-4967-a56a-c0dcf30f80da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "tracer_provider = TracerProvider()\n",
    "cloud_trace_exporter = CloudTraceSpanExporter()\n",
    "tracer_provider.add_span_processor(\n",
    "    # BatchSpanProcessor buffers spans and sends them in batches in a\n",
    "    # background thread. The default parameters are sensible, but can be\n",
    "    # tweaked to optimize your performance\n",
    "    BatchSpanProcessor(cloud_trace_exporter)\n",
    ")\n",
    "trace.set_tracer_provider(tracer_provider)\n",
    "\n",
    "tracer = trace.get_tracer(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fdb3b705-206b-4d79-8a2b-ef71e8ec0edb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from vertexai.language_models import TextEmbeddingModel\n",
    "\n",
    "def text_embedding(tracer_object, text: str = \"What is life?\") -> list:\n",
    "    \"\"\"Text embedding with a Large Language Model.\"\"\"\n",
    "    with tracer.start_as_current_span(\"text_embeddings\", links=[Link(tracer_object.context)]) as text_embeddings:\n",
    "        model = TextEmbeddingModel.from_pretrained(\"textembedding-gecko@003\")\n",
    "        embeddings = model.get_embeddings([text])\n",
    "        vector = embeddings[0].values\n",
    "        #print(f\"Length of Embedding Vector: {len(vector)}\")\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3da06372-560f-4f48-8f43-166c167770e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "db_user = os.environ['DB_USER']\n",
    "db_pass = os.environ['DB_PASSWORD']\n",
    "db_host = os.environ['DB_HOST']\n",
    "db_name = 'postgres'\n",
    "db_port = '5432'  # Default PostgreSQL port\n",
    "table_name = 'embeddings_sample'  # Table where you want to store the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f1d6bfc-53e6-499a-bf47-0975891b75a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import psycopg2\n",
    "\n",
    "def get_documents(tracer_object, vector):\n",
    "    with tracer.start_as_current_span(\"get_documents\", links=[Link(tracer_object.context)]) as get_documents:\n",
    "        \n",
    "\n",
    "        conn = psycopg2.connect(dbname=db_name, user=db_user, password=db_pass, host=db_host, port=db_port)\n",
    "\n",
    "        # Assuming `embeddings` is a list of embedding vectors you received from the textembedding service\n",
    "        # And `texts` is the list of texts corresponding to each embedding\n",
    "        #for text, embedding in zip(texts, embeddings):\n",
    "            # Convert the embedding to a format suitable for storage, e.g., a string\n",
    "            #embedding_str = f\"[{','.join(map(str, str(embedding)))}]\"\n",
    "        #print(embeddings)\n",
    "            # Insert into database\n",
    "        results = None\n",
    "        with conn.cursor() as cur:\n",
    "            cur.execute(f\"\"\"SELECT DISTINCT content, 1 - (embedding <=> '{vector}') AS similarity FROM embeddings_sample\n",
    "                         WHERE 1 - (embedding <=> '{vector}') > .72\n",
    "                         ORDER BY similarity DESC\n",
    "                         LIMIT 10\"\"\")\n",
    "            results = cur.fetchall()\n",
    "\n",
    "        conn.close()\n",
    "                    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a3993c12-f335-49fa-b2d3-996220e95014",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# langchain_integration.py\n",
    "\n",
    "def get_billing_data_for_project(tracer_object, project_id):\n",
    "    \n",
    "    with tracer.start_as_current_span(\"get_cost\", links=[Link(tracer_object.context)]) as get_cost:\n",
    "    \n",
    "        conn = psycopg2.connect(dbname=db_name, user=db_user, password=db_pass, host=db_host, port=db_port)\n",
    "\n",
    "        # Query the database for billing data related to the specified project\n",
    "        query = \"\"\"\n",
    "        SELECT service_name, SUM(cost) as total_cost\n",
    "        FROM gcp_billing_data\n",
    "        WHERE project_id = %s\n",
    "        GROUP BY service_name\n",
    "        \"\"\"\n",
    "        with conn.cursor() as cursor:\n",
    "            cursor.execute(query, (project_id,))\n",
    "            results = cursor.fetchall()\n",
    "        conn.close()\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a3e45174-b819-4980-97cc-fae6b78f5df3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import SequentialChain, LLMChain\n",
    "from IPython.display import display, Markdown\n",
    "from langchain.llms import VertexAI\n",
    "\n",
    "def llm_request(tracer_object, question):\n",
    "    \n",
    "    with tracer.start_as_current_span(\"main_llm\", links=[Link(tracer_object.context)]) as main_llm:\n",
    "        \n",
    "        model = VertexAI(model_name=\"gemini-pro\", temperature=0.5, max_output_tokens=2048)\n",
    "        \n",
    "        # ==== Get project id ====\n",
    "        project_template = \"\"\"identify the google cloud project id in the following text: \n",
    "        {question}\n",
    "        \"\"\"\n",
    "        \n",
    "        project_prompt = PromptTemplate(template=project_template, input_variables=[\"question\"])\n",
    "        \n",
    "        project_chain = LLMChain(llm=model,\n",
    "                                  prompt=project_prompt,\n",
    "                                  output_key=\"project_id\",\n",
    "                                 )\n",
    "\n",
    "        \n",
    "        with tracer.start_as_current_span(\"llm_project_id\", links=[Link(tracer_object.context)]) as llm_project_id:\n",
    "            \n",
    "            tracer_object.set_attribute(\"prompt length\", len('\\n'.join(question)))\n",
    "\n",
    "            project_answer = project_chain({\n",
    "                \"question\": question,\n",
    "                })\n",
    "            \n",
    "            \n",
    "        \n",
    "        # === Retrieve data for the project ===\n",
    "        \n",
    "        #project_data = get_billing_data_for_project(tracer_object, project_answer['project_id'])\n",
    "        \n",
    "        \n",
    "        # === Summarize the data ====\n",
    "        summary_template = \"\"\"in a moment I will provide you with a set of text. Summarize the main points of these texts and include any expertise DoiT can provide.\n",
    "        \n",
    "        {chunks}\n",
    "        \"\"\"\n",
    "        \n",
    "        summary_prompt = PromptTemplate(template=summary_template, input_variables=[\"chunks\"])\n",
    "\n",
    "        summary_chain = LLMChain(llm=model,\n",
    "                                  prompt=summary_prompt,\n",
    "                                  output_key=\"context\",\n",
    "                                 )\n",
    "        \n",
    "        #with tracer.start_as_current_span(\"llm_summary_invoke\", links=[Link(tracer_object.context)]) as llm_summary_invoke:\n",
    "            \n",
    "            #tracer_object.set_attribute(\"prompt length\", len('\\n'.join(matches)))\n",
    "\n",
    "            #summary_answer = summary_chain.({\n",
    "            #    \"chunks\": matches,\n",
    "            #    })\n",
    "            \n",
    "        \n",
    "        # === Retrieve context data ====\n",
    "        vector = text_embedding(tracer_object, \"how can cost be manage in google cloud?\")\n",
    "\n",
    "        matches = get_documents(tracer_object, vector)\n",
    "\n",
    "        matches = '\\n'.join([match[0] for match in matches])\n",
    "        \n",
    "        \n",
    "            \n",
    "        #display(Markdown(summary_answer))\n",
    "        \n",
    "        # === Retrieve analysis ===\n",
    "        \n",
    "        template = \"\"\"This is information on how to do cost analysis\n",
    "        \n",
    "                    {context}\n",
    "        \n",
    "                    In a moment I will give you data for the cost of service in a specific google project. Using the following information to help create a cost analysis:\n",
    "                    \n",
    "                    cost data for {project_id}\n",
    "                    \n",
    "                    {project_data}\n",
    "                    \n",
    "                    Question: {question}\n",
    "                    \"\"\"\n",
    "    \n",
    "    \n",
    "        prompt = PromptTemplate(template=template, input_variables=[\"context\", \"project_id\", \"project_data\", \"question\"])\n",
    "\n",
    "        chain = LLMChain(llm=model,\n",
    "                          prompt=prompt,\n",
    "                          output_key=\"analysis\",\n",
    "                         )\n",
    "        \n",
    "        with tracer.start_as_current_span(\"llm_invoke\", links=[Link(tracer_object.context)]) as llm_invoke:\n",
    "            \n",
    "            #tracer_object.set_attribute(\"prompt length\", len(summary_answer) + len(question))\n",
    "\n",
    "            overall_chain = SequentialChain(\n",
    "                chains=[summary_chain, chain],\n",
    "                input_variables=[\"question\", \"chunks\", \"project_data\", \"project_id\"],\n",
    "                output_variables=[\"context\", \"analysis\"]\n",
    "            )\n",
    "            \n",
    "            answer = overall_chain({\"question\": question,\n",
    "                                    \"chunks\": matches,\n",
    "                                    \"project_data\": get_billing_data_for_project(tracer_object, project_answer['project_id']),\n",
    "                                    \"project_id\": project_answer['project_id']\n",
    "                                   })\n",
    "\n",
    "    return answer['analysis']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "af5dd79e-b9c3-42a8-a590-c9e3252f433b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import decimal\n",
    "\n",
    "class DecimalEncoder(json.JSONEncoder):\n",
    "    def default(self, o):\n",
    "        if isinstance(o, decimal.Decimal):\n",
    "            return str(o)\n",
    "        return super().default(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62177087-96ad-49c2-acca-32a846589314",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "114f28e1-1acc-4f73-94c2-459de7083c5f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Cost Analysis for Project proj-webapp-3**\n",
       "\n",
       "**Cost Allocation:**\n",
       "\n",
       "The project's total cost is the sum of the individual service costs:\n",
       "\n",
       "```\n",
       "Total Cost = Cloud Storage + Cloud CDN + Cloud SQL + App Engine + Identity Platform + Cloud Load Balancing + Cloud Firestore + Cloud Pub/Sub + Cloud Functions + Compute Engine\n",
       "```\n",
       "\n",
       "```\n",
       "Total Cost = 32.5000 + 41.9700 + 49.8100 + 9.0500 + 44.3600 + 19.3500 + 13.8200 + 32.0100 + 65.7300 + 39.9100\n",
       "```\n",
       "\n",
       "```\n",
       "Total Cost = 348.5100\n",
       "```\n",
       "\n",
       "**Cost Optimization:**\n",
       "\n",
       "* **Cloud Storage:** Consider using a more cost-effective storage class or optimizing object lifecycle management.\n",
       "* **Cloud CDN:** Explore optimizing cache policies and reducing bandwidth usage.\n",
       "* **Cloud SQL:** Review database usage patterns and consider using a smaller instance or optimizing database performance.\n",
       "* **App Engine:** Analyze application performance and consider scaling down instances or optimizing code.\n",
       "* **Identity Platform:** Review user authentication patterns and consider optimizing access controls.\n",
       "* **Cloud Load Balancing:** Ensure that load balancers are configured optimally for traffic volume.\n",
       "* **Cloud Firestore:** Optimize data structures and indexing to reduce read and write costs.\n",
       "* **Cloud Pub/Sub:** Review message volume and consider using a more cost-effective pricing tier.\n",
       "* **Cloud Functions:** Analyze function execution time and consider optimizing code or reducing function invocations.\n",
       "* **Compute Engine:** Review instance usage patterns and consider using spot instances or preemptible VMs.\n",
       "\n",
       "**Cost Governance:**\n",
       "\n",
       "* Establish cost budgets and alerts to monitor spending and prevent overages.\n",
       "* Implement automated cost allocation mechanisms to track costs by department or project.\n",
       "* Regularly review cost reports and identify areas for further optimization.\n",
       "\n",
       "**Cloud Cost Analytics:**\n",
       "\n",
       "* Use tools like DoiT Cloud Analytics to analyze billing data, identify cost drivers, and track optimization efforts.\n",
       "* Monitor cost trends over time and identify any anomalies or unexpected increases.\n",
       "\n",
       "**External Expertise:**\n",
       "\n",
       "* Consider consulting with cloud partners like DoiT to enhance cost management, analytics, and governance.\n",
       "* Leverage their expertise in cost optimization, automated governance, and advanced hardware solutions."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "with tracer.start_as_current_span(\"qa_llm_evaluation\") as llm_evaluation:\n",
    "    query = \"Analyze the cost of the project proj-webapp-3?\"\n",
    "\n",
    "    answer = llm_request(llm_evaluation, query)\n",
    "    \n",
    "    llm_evaluation.set_attribute(\"output length\", len(answer))\n",
    "    \n",
    "    llm_evaluation.add_event(name=\"received output\")\n",
    "\n",
    "    display(Markdown(answer))\n",
    "    \n",
    "    llm_evaluation.add_event(name=\"display output\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5198bc-729c-4a32-b9c8-9993c1e74477",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-root-py",
   "name": "workbench-notebooks.m118",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m118"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
